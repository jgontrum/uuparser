{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T20:53:51.014282Z",
     "start_time": "2018-11-04T20:53:49.941468Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=0)\n",
    "\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea for the training corpus: \n",
    "\n",
    "Randomly generate sets of vectors and try to learn their average.\n",
    "\n",
    "Generate 100000 (_number_of_examples_) samples of 2 (_number_of_vecors_) vectors that have a length of 10 (_vector_length_), then compute (as labels) their average. The learned weights for each vector should then be each 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T20:56:52.968720Z",
     "start_time": "2018-11-04T20:56:52.946572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate corpus\n",
    "number_of_examples = 250\n",
    "vector_length = 10\n",
    "number_of_vecors = 2\n",
    "\n",
    "X = np.random.rand(\n",
    "    number_of_examples,\n",
    "    number_of_vecors, \n",
    "    vector_length\n",
    ")\n",
    "\n",
    "y = np.array([np.mean(x, axis=0) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T20:57:16.112919Z",
     "start_time": "2018-11-04T20:57:15.678575Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.43455055401\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n",
      "1.43399528363e-12\n",
      "[0.49999997 0.5       ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I have no idea why exactly the computational graph has to be renewed.\n",
    "dy.renew_cg();\n",
    "\n",
    "# The model keeps track of all the variables that we want to update.\n",
    "model = dy.ParameterCollection()\n",
    "\n",
    "# Creat a different weight parameter per layer\n",
    "weights = [\n",
    "    model.add_parameters(1, init=0.12345, name=\"weight-%s\" % i)\n",
    "    for i in range(number_of_vecors)\n",
    "]\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "for epoch in range(20):\n",
    "    cum_loss = []\n",
    "    for i in range(number_of_examples):\n",
    "        dy.renew_cg();\n",
    "\n",
    "        # Wrap everything in a tensor for dynet, because it needs\n",
    "        # everything to be in this format.\n",
    "        y_ = dy.inputTensor(y[i])\n",
    "        \n",
    "        # Iterate over \"layer\" and its weight parameter and multiply them\n",
    "        y_hat = [\n",
    "            dy.inputTensor(layer) * weight\n",
    "            for layer, weight in zip(X[i], weights)\n",
    "        ]\n",
    "        # Sum the layer contents together\n",
    "        y_hat = dy.esum(y_hat)\n",
    "    \n",
    "        # Compute loss\n",
    "        loss = dy.squared_distance(y_hat, y_)\n",
    "        \n",
    "        # Save the loss so that we can see if each epoch is getting better\n",
    "        cum_loss.append(loss.value())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "\n",
    "    \n",
    "    print(sum(cum_loss))\n",
    "    print(np.array([w.value() for w in weights]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynet",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
